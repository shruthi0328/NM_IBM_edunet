VISUAL QUESTION & ANSWERING USING TRANSFORMER LIBRARY
Description:
This project utilizes the powerful capabilities of the Transformers library to perform Visual Question & Answering (VQA). It combines state-of-the-art natural language processing with computer vision techniques to answer questions based on images. The model is trained on a dataset of images paired with corresponding questions and answers, enabling it to understand and respond to a wide range of inquiries about visual content.
1. Introduction:
   Visual Question & Answering (VQA) is an interdisciplinary research area that involves answering questions about images. This project leverages the Transformers library, which provides pre-trained models for natural language processing tasks, to develop a VQA system. By combining text-based queries with image inputs, the model can generate accurate responses to a variety of questions. 
2. Usage:
   To use the Visual Question & Answering system:
   - Run the main script, specifying the input image and question:
     python vqa.py --image <image_path> --question "What is in the image?"
   - The model will process the image and question, and provide the corresponding answer.




